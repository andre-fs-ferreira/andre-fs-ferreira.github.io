<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="/XSL/publications.xsl"?>
<publications homepage_aria_label="Homepage">
  <header>
    <title>Publications</title>
    <subtitle>9 first author publications + 12 Co-author publications</subtitle>
  </header>

  <year value="2025">
    <author-group type="first" title="First author">
      <publication>
        <title>Achieving Over 10× Faster Sample Generation with conditional DDPM</title>
        <description>We present a unified denoising-diffusion framework for BraTS 2025 Task 8 (synthesizing missing MRI modalities) and Task 9 (inpainting pathology-free regions), with a fast inference strategy over 10× quicker than our BraTS 2024 solution while keeping low computational cost. Results — Task 8: Dice 0.86, SSIM 0.77; Task 9: RMSE 0.053, PSNR 26.77, SSIM 0.918. Code to be released soon.</description>
        <keywords>Conditional Denoising Diffusion, IDDPM, Synthetic Data Generation, MRI Reconstruction, Computational Efficiency, Image Inpainting</keywords>
        <image src="/imgs/2025/Brats_2025_task8_9.png"/>
      </publication>
    </author-group>
    <author-group type="mid" title="Co-author">
      <publication>
        <title><![CDATA[<a href="https://arxiv.org/pdf/2601.16759"> Curated endoscopic retrograde cholangiopancreatography images dataset </a>]]></title>
        <description>Endoscopic Retrograde Cholangiopancreatography (ERCP) is an essential procedure for diagnosing and treating biliary and pancreatic diseases. Although artificial intelligence offers promising solutions for automated diagnosis, the lack of publicly available ERCP datasets limits progress in this field. To address this gap, this study presents a large, curated ERCP image dataset comprising 19,018 raw images and 19,317 processed images from 1,602 patients. Among these, 5,519 images are labeled and ready for immediate use. All images were manually reviewed and annotated by two experienced gastroenterologists and validated by a senior specialist, ensuring high-quality standards. The dataset’s utility was demonstrated through a classification experiment, positioning it as a potential benchmark for automated ERCP analysis and diagnosis.</description>
        <keywords> ERCP; Classification; DataSet</keywords>
        <image src="/imgs/2025/ERCP_dataset.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://www.mdpi.com/1718-7729/32/10/587"> Skeletal Muscle Radiation Attenuation at C3 Predicts Survival in Head and Neck Cancer </a>]]></title>
        <description>Our study demonstrates a practical and powerful way to predict outcomes in head and neck cancer (HNC). While low muscle mass is a known risk factor, it's typically measured at the L3 vertebra, which is missed in HNC scans. Using a deep learning analysis of 904 patients, we validate that measurements at the C3 vertebra—an area already included in routine scans—are highly prognostic. We found that both low muscle quantity (SMA) and low muscle quality (SMRA) at C3 independently correlate with worse locoregional control and overall survival, establishing C3 muscle metrics as a critical, accessible new biomarker for HNC.</description>
        <keywords> Head and neck neoplasms, Skeletal muscle, Myosteatosis; Survival analysis; Tomography</keywords>
        <image src="/imgs/2025/C3_Predicts.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://arxiv.org/pdf/2405.18383"> Analysis of the 2024 BraTS Meningioma Radiotherapy Planning Automated Segmentation Challenge </a>]]></title>
        <description> The 2024 BraTS-MEN-RT challenge advanced automated segmentation for meningioma radiotherapy using a 750-case multi-institutional MRI dataset with expert GTV annotations. Six teams developed and evaluated models using modified lesion-wise DSC and 95HD metrics, with the best achieving 0.815 DSC and 26.92 mm 95HD. The challenge provides a strong foundation for improving precision and personalization in radiotherapy planning. </description>
        <keywords> Meningioma; BraTS; Machine Learning; Segmentation; BraTS-Meningioma; Image Analysis Challenge; Artificial Intelligence; AI; Radiation Oncology; Radiotherapy; Stereotactic Radiosurgery; Gamma Knife</keywords>
        <image src="/imgs/2025/BraTS_Meningioma.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://www.sciencedirect.com/science/article/pii/S2352340925000198"> GBM-Reservoir: Brain tumor (Glioblastoma Multiforme) MRI dataset collection with ground truth segmentation masks </a>]]></title>
        <description>We present a brain tumor MRI dataset with 23,049 samples, each including FLAIR, T1, T1ce, and T2 scans, plus one or two segmentation masks. The dataset expands 438 original BraTS 2022 cases through registration, generating additional samples with varied tumor locations. Despite heterogeneous imaging protocols, it provides ground truth for each sample, supporting the development of deep learning–based automated tumor segmentation on unseen cases.</description>
        <keywords>Brain tumor segmentation, Data augmentation, Registration, BraTS, Deep learning</keywords>
        <image src="/imgs/2025/GBM-Reservoir.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://www.sciencedirect.com/science/article/pii/S0957417425036474">Beyond Benchmarks: Towards Robust Artificial Intelligence Bone Segmentation in Socio-Technical Systems</a>]]></title>
        <description>We evaluated 20 state-of-the-art mandibular segmentation models on 19,218 segmentations from 1,000 multicenter CT/CBCT scans and found accuracy can vary by up to 25% due to factors like voxel size, bone orientation, and patient conditions. While smaller isotropic voxels and neutral orientation improved performance, metallic osteosynthesis and complex anatomy degraded it. These results highlight that AI models are not “plug-and-play” and provide evidence-based recommendations to improve clinical integration.</description>
        <keywords>Artificial Intelligence, Image Processing, Computer-Assisted Imaging, Three-Dimensional, 50 Tomography, X-Ray Computed, Practice Guideline, Mandible</keywords>
        <image src="/imgs/2025/Beyond_Benchmarks.png"/>
      </publication>
    </author-group>
  </year>

  <year value="2024">
    <author-group type="first" title="First author">
      <publication>
        <title><![CDATA[<a href="https://arxiv.org/abs/2411.04632v1?utm_source=tldrai">Improved Segmentation with Synthetic Data</a>]]></title>
        <description><![CDATA[We present our winning Task 1 and third-place Task 3 solutions for BraTS, using synthetic data to train SOTA segmentation pipelines—improving robustness for post-treatment glioma segmentation though the pipeline was less suited to meningioma. Results — Task 1 (ET, NETC, RC, SNFH, TC, WT): DSC = 0.790, 0.808, 0.776, 0.893, 0.787, 0.894; HD95 = 35.63, 30.35, 44.58, 16.87, 38.19, 17.95. Task 3 (test): DSC = 0.801, HD95 = 38.26. Code available on <a href="https://github.com/ShadowTwin41/BraTS_2023_2024_solutions">BraTS_2023_2024_solutions</a>.]]></description>
        <keywords>Brain Tumour Segmentation, Synthetic data, nnUnet, MedNeXt, Swin-UNETR</keywords>
        <image src="/imgs/2024/BraTS_2024_task1_3.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://arxiv.org/abs/2411.04630">Brain Tumour Removing and Missing Modality Generation using 3D WDM</a>]]></title>
        <description><![CDATA[We present our second-place solution for BraTS 2024 Task 8 (and a participation entry for Task 7) that uses conditional 3D wavelet diffusion models to handle lesions and missing MRI modalities. By applying a wavelet transform we train and infer at full resolution on a 48 GB GPU without patching or downsampling—preserving all image information and improving prediction reliability; code is available on <a href="https://github.com/ShadowTwin41/BraTS_2023_2024_solutions">BraTS_2023_2024_solutions</a>.]]></description>
        <keywords>3D WDM, MRI, Brain Tumour, Inpainting, Missing Modality</keywords>
        <image src="/imgs/2024/BraTS_2024_task7_8.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://arxiv.org/abs/2402.17317">How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation</a>]]></title>
        <description><![CDATA[We address limited medical data for BraTS 2023 Task 1 by using GANs and registration to augment training samples for three models: nnU-Net, Swin UNETR, and the BraTS 2021 winning solution. Combining convolutional and transformer architectures enhances performance. On the validation set, our best model achieves Dice = 0.901, 0.867, 0.851 and HD95 = 14.94, 14.47, 17.70 for whole tumor, tumor core, and enhancing tumor, respectively. Code is available on <a href="https://github.com/ShadowTwin41/BraTS_2023_2024_solutions">BraTS_2023_2024_solutions</a>.]]></description>
        <keywords>Generative adversarial networks, Registration, Synthetic data, Brain Tumour segmentation, nnU-Net</keywords>
        <image src="/imgs/2024/BraTS_2023.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://ieeexplore.ieee.org/document/10635839">Generalisation of segmentation using Generative Adversarial Networks</a>]]></title>
        <description>We tackle robustness in BraTS 2024 GoAT by using conditional GANs to generate realistic new cases and train a segmentation model that combines convolutions with attention mechanisms. On the validation set we achieve DSC = 0.855 (enhancing tumor), 0.863 (tumor core), 0.883 (whole tumor), and HD95 = 24.83, 24.10, 21.72, respectively.</description>
        <keywords>Generative Adversarial Networks, Synthetic data, nnU-Net, Swin UNETR, Segmentation</keywords>
        <image src="/imgs/2024/BraTS_2024_Goat.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://www.sciencedirect.com/science/article/pii/S1361841524000252">GAN-based generation of realistic 3D volumetric data: A systematic review and taxonomy</a>]]></title>
        <description>We review GAN-based methods for generating realistic volumetric (3D) data—primarily in medicine—motivated by limited datasets caused by rarity, privacy, and high acquisition cost. We summarize common architectures, loss functions and evaluation metrics, present a novel taxonomy, compare advantages and drawbacks, and outline evaluations, open challenges, and research opportunities to give a holistic overview of volumetric GANs.</description>
        <keywords>Synthetic volumetric data, Generative adversarial network, Systematic review, Volumetric GANs taxonomy</keywords>
        <image src="/imgs/2024/GANs_review.png"/>
      </publication>
    </author-group>
    <author-group type="mid" title="Co-author">
      <publication>
        <title><![CDATA[<a href="https://link.springer.com/chapter/10.1007/978-3-031-83274-1_10">Comparative Analysis of nnUNet and MedNeXt for Head and Neck Tumor Segmentation in MRI-Guided Radiotherapy</a>]]></title>
        <description><![CDATA[We address limited medical data for BraTS 2023 Task 1 by using GANs and registration to massively augment training samples for three models: nnU-Net, Swin UNETR, and the BraTS 2021 winning solution. Combining convolutional and transformer architectures improves complementarity. Results on the test set — lesion-wise Dice: 0.885, 0.872, 0.869; lesion-wise HD95: 22.84, 22.97, 16.71 (WT, TC, ET). Code available on <a href="https://github.com/NikooMoradi/HNTSMRG24_team_TUMOR">HNTSMRG24_team_TUMOR</a>.]]></description>
        <keywords>HNTS-MRG24, MICCAI24, nnUNet, MedNeXt</keywords>
        <image src="/imgs/2024/HNTS-MRG24.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://arxiv.org/abs/2407.01318">Deep Dive Into MRI: Exploring Deep Learning Applications in 0.55T and 7T MRI</a>]]></title>
        <description>We review the integration of deep learning into emerging 0.55T and 7T MRI technologies, highlighting how DL enhances image detail, tissue characterization, and diagnostic performance. The paper discusses current applications, benefits, and future directions for combining deep learning with advanced MRI to further improve medical imaging.</description>
        <keywords>Magnetic Resonance Imaging, 0.55T MRI, 7T MRI, Deep Learning</keywords>
        <image src="/imgs/2024/Deep-dive_MRI.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://link.springer.com/article/10.1007/s00784-024-05781-5">Insights into Predicting Tooth Extraction from Panoramic Dental Images: Artificial Intelligence vs. Dentists</a>]]></title>
        <description>We trained a ResNet50 model on 26,956 cropped teeth images from panoramic radiographs to predict whether a tooth should be extracted or preserved. The AI achieved ROC-AUC = 0.901 and PR-AUC = 0.749, outperforming dentists (ROC-AUC = 0.797, PR-AUC = 0.589). These results show that AI can support clinical decision-making by improving accuracy and reducing errors in tooth extraction indications.</description>
        <keywords>Tooth Extraction, Surgery, Oral Dentistry, Decision Support Techniques, Deep Learning, Artificial Intelligence</keywords>
        <image src="/imgs/2024/panoramic_tooth.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://ieeexplore.ieee.org/document/10635337">A Baseline Solution for the ISBI 2024 Dreaming Challenge</a>]]></title>
        <description>We introduce the DREAMING challenge, which explores Diminished Reality for medicine by removing surgical tools from operation scenes to enhance visibility. A new dataset of simulated surgeries with instrument and hand occlusions is presented, and a baseline using an existing video inpainting model is evaluated. Results show promising performance under some conditions but highlight key limitations for real-world surgical use.</description>
        <keywords>Deep Learning, Inpainting, Diminished Reality, Surgery</keywords>
        <image src="/imgs/2024/Baseline_Dreaming_Challenge.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://arxiv.org/abs/2402.04301">Deep PCCT: Photon Counting Computed Tomography Deep Learning Applications Review</a>]]></title>
        <description>This review explores recent advances and applications of Photon Counting Computed Tomography (PCCT) in pre-clinical research, highlighting its ability to overcome traditional imaging limitations like low resolution and noise. It analyzes scanner features, deep learning integration, and radiomic applications, showing PCCT’s promise for improved diagnostic precision while outlining current challenges and future research directions in medical imaging.</description>
        <keywords>Photon Counting Computed Tomography, Deep Learning, Radiomics</keywords>
        <image src="/imgs/2024/Deep_PCCT.png"/>
      </publication>
    </author-group>
  </year>

  <year value="2023">
    <author-group type="first" title="First author">
      <publication>
        <title><![CDATA[<a href="https://link.springer.com/chapter/10.1007/978-3-031-76163-8_8">Enhanced Data Augmentation using Synthetic Data for Brain Tumour Segmentation</a>]]></title>
        <description><![CDATA[We address limited medical data for BraTS 2023 Task 1 by using GANs and registration to massively augment training samples for three models: nnU-Net, Swin UNETR, and the BraTS 2021 winning solution. Combining convolutional and transformer architectures improves complementarity. Results on the test set — lesion-wise Dice: 0.885, 0.872, 0.869; lesion-wise HD95: 22.84, 22.97, 16.71 (WT, TC, ET). Code available on <a href="https://github.com/ShadowTwin41/BraTS_2023_2024_solutions">BraTS_2023_2024_solutions</a>.]]></description>
        <keywords>Generative adversarial networks, Registration, Synthetic data, Brain Tumour segmentation, nnU-Net</keywords>
        <image src="/imgs/2024/BraTS_2023.png"/>
      </publication>
    </author-group>
    <author-group type="mid" title="Co-author">
      <publication>
        <title><![CDATA[<a href="https://arxiv.org/abs/2308.16139">MedShapeNet – a large-scale dataset of 3D medical shapes for computer vision</a>]]></title>
        <description><![CDATA[We introduce MedShapeNet, a large-scale collection of over 100,000 annotated 3D medical shapes derived from real patient imaging, covering anatomy and surgical instruments. Designed to bridge computer vision and medical imaging, it supports tasks such as tumor classification, skull reconstruction, anatomy completion, education, and 3D printing. The data is openly accessible via a web and Python interface, enabling research in discriminative, reconstructive, and variational benchmarks, as well as extended reality applications. The project page is: <a href="https://medshapenet.ikim.nrw/">https://medshapenet.ikim.nrw/</a>.]]></description>
        <keywords>3D medical shapes, benchmark, anatomy education, shapeomics, augmented reality, virtual reality</keywords>
        <image src="/imgs/2023/medshapenet.webp"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://www.sciencedirect.com/science/article/pii/S2352711023001280">Open-source skull reconstruction with MONAI</a>]]></title>
        <description><![CDATA[We propose an autoencoder-based deep learning model for cranial and facial defect reconstruction within the MONAI framework, pre-trained on the MUG500+ and SkullFix datasets. The work emphasizes open-sourcing, providing accessible code and pre-trained weights via <a href="https://github.com/Project-MONAI/research-contributions/tree/master/SkullRec/">MONAI’s official repository</a>. This contribution introduces a pre-trained reconstruction model and demonstrates how MONAI tutorials can be easily adapted for new medical imaging applications like skull reconstruction.]]></description>
        <keywords>Skull reconstruction, Research contribution, MONAI, Open-source, API, PyTorch, Python, Deep learning, Pre-trained model, Cranial implant design, Cranioplasty, Craniotomy, Craniectomy, CT, BoneHeadFace</keywords>
        <image src="/imgs/2023/Open-source-skull.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://link.springer.com/chapter/10.1007/978-3-031-45642-8_29">Generation of Synthetic X-Rays Images of Rib Fractures Using a 2D Enhanced Alpha-GAN for Data Augmentation</a>]]></title>
        <description>We developed a generative model to synthesize X-ray images with subtle rib fractures, addressing the scarcity of public datasets limited by ethical and bureaucratic constraints. Although evaluated with quantitative metrics and a Turing test, the generated images lacked sufficient realism due to high dataset heterogeneity, highlighting challenges in modeling fine fracture details.</description>
        <keywords>alpha-GAN, data augmentation, X-ray, fractures, rib</keywords>
        <image src="/imgs/2023/synthetic-x-rays.jpg"/>
      </publication>
    </author-group>
  </year>

  <year value="2022">
    <author-group type="first" title="First author">
      <publication>
      
        <title><![CDATA[<a href="https://www.mdpi.com/2076-3417/12/10/4844">Generation of Synthetic Rat Brain MRI Scans with a 3D Enhanced Alpha Generative Adversarial Network</a>]]></title>
        <description><![CDATA[We adapt an α-GAN to generate realistic 3D rat-brain MRIs to overcome scanner scarcity, long acquisition times, and data/privacy constraints. To our knowledge this is the first GAN-based generation of rat MRI; with a new normalization layer and loss terms, we validate realism via quantitative metrics, a Turing test, and a segmentation task — and show that training with our synthetic scans improves segmentation more than conventional augmentation. Code available on <a href="https://github.com/ShadowTwin41/alpha-WGAN-SigmaRat">alpha-WGAN-SigmaRat</a>.]]></description>
        <keywords>Alpha Generative Adversarial Network, Data Augmentation, Synthetic Data, MRI rat brain</keywords>
        <image src="/imgs/2022/SigmaRat.png"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://www.igi-global.com/chapter/generation-of-synthetic-data/301776">Generation of Synthetic Data: A Generative Adversarial Networks Approach</a>]]></title>
        <description>We adapt an α-GAN to generate unlimited synthetic 3D rat-brain MRIs as a low-cost way to overcome scarce, unrepresentative medical datasets that traditional augmentation cannot solve. We validate realism by visual assessment and demonstrate practical value by improving performance in a segmentation test.</description>
        <keywords>Alpha Generative Adversarial Network, Data Augmentation, Deep Learning Models, 3D MRI data sets of rat brain</keywords>
        <image src="/imgs/2022/IGI-SigmaRat.png"/>
      </publication>
    </author-group>
    <author-group type="mid" title="Co-author">
      <publication>
        <title><![CDATA[<a href="https://www.mdpi.com/2075-4418/12/11/2733">Radiomics in Head and Neck Cancer Outcome Predictions</a>]]></title>
        <description>We use radiomics and machine learning to predict head and neck cancer prognosis—including locoregional recurrences, distant metastases, and overall survival—by extracting features from CT images combined with patient clinical data. Using XGBoost, our models achieved AUC = 0.74 (LR), 0.84 (DM), 0.91 (OS), demonstrating that CT-based radiomic features can effectively support prognosis and clinical decision-making.</description>
        <keywords>precision medicine, head and neck cancer, radiomics, locoregional recurrences, distant metastases, overall survival, CT, multilayer perceptron, XGBoost</keywords>
        <image src="/imgs/2022/radiomics_survival.jpg"/>
      </publication>
      <publication>
        <title><![CDATA[<a href="https://github.com/heiligerl/AutoPET_Challenge_Submission">AutoPET Challenge: Combining nn-Unet with Swin UNETR Augmented by Maximum Intensity Projection Classifier</a>]]></title>
        <description><![CDATA[We present an ensemble solution for the AutoPET challenge, combining nnU-Net and Swin UNETR with a maximum intensity projection classifier that gates lesion detection, followed by late fusion of segmentations. Tested on FDG-PET/CT scans from 900 patients, our approach achieves an average cross-validation Dice score of 72.12% for lung cancer, melanoma, and lymphoma. Code is available on <a href="https://github.com/heiligerl/AutoPET_Challenge_Submission">AutoPET_Challenge_Submission</a>.]]></description>
        <keywords>Semantic Segmentation, FDG-PET/CT, Swin UNETR, nn-Unet, Late Fusion</keywords>
        <image src="/imgs/2022/AutoPET.png"/>
      </publication>
    </author-group>
  </year>
</publications>
